{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "file = open(\"DATA.txt\", \"r\",encoding='utf-8')\n",
    "raw = [t for t in file.readlines()]\n",
    "raw = raw[3:].copy()\n",
    "rawtext = \"\".join(raw)\n",
    "\n",
    "# 시간 추출\n",
    "time_p = re.compile('\\[(오전 |오후 )+([^]]+)\\]') # 시간 추출 패턴\n",
    "times = re.finditer(time_p, rawtext)\n",
    "times_loc = [e.span() for e in times]\n",
    "times = re.finditer(time_p, rawtext)\n",
    "times_txt = [e.group().replace(\"[\", \"\")[:-1] for e in times]\n",
    "\n",
    "#날짜 추출\n",
    "date_p = re.compile(\"\\d{4}년\\s\\d{1,2}월\\s\\d{1,2}일\\s\\S+요일\") # 날짜 추출 패턴\n",
    "dates = re.finditer(date_p, rawtext)\n",
    "dates_loc = [e.span() for e in dates]\n",
    "dates = re.finditer(date_p, rawtext)\n",
    "dates_txt = [e.group() for e in dates]\n",
    "\n",
    "# 유저명 추출\n",
    "name_p = re.compile('\\[([^]]+)\\] \\[오') # 유저명 추출 패턴\n",
    "names = re.finditer(name_p, rawtext)\n",
    "names_loc = [e.span() for e in names] # 위치 추출\n",
    "names = re.finditer(name_p, rawtext)\n",
    "names_txt = [e.group()[:-2] for e in names] # 텍스트 추출\n",
    "names_txt = [e.replace(\"[\", \"\")[:-2] for e in names_txt]\n",
    "\n",
    "# 메세지 추출\n",
    "texts = []\n",
    "for i, tl in enumerate(times_loc):\n",
    "    tl = tl[1]+1\n",
    "    try:\n",
    "        nl = names_loc[i+1][0]-1\n",
    "    except:\n",
    "        nl = len(rawtext)\n",
    "    texts.append((tl, rawtext[tl:nl])) # 시간 패턴의 끝 위치부터 다음 유저명 패턴 시작 전 위치([오 텍스트 빼려고) 까지가 메세지\n",
    "\n",
    "df = pd.DataFrame(texts) # 데이터프레임 생성\n",
    "dates_text = []\n",
    "for num in df[0]: # 저장한 캐릭터 번호를 기준으로 날짜 부여\n",
    "    for dt, dtl in zip(dates_txt, dates_loc): \n",
    "        if num >= dtl[1]: # 날짜 텍스트 위치의 끝번호보다 캐릭터 번호가 크면\n",
    "            rdt = dt # dt값을 해당 날짜를 기준으로 업데이트\n",
    "        elif num < dtl[1]:  # 날짜 텍스트 위치의 끝번호보다 캐릭터 번호가 작으면\n",
    "            break\n",
    "    dates_text.append(rdt) # 메세지 별 날짜 저장 \n",
    "\n",
    "df['date']=dates_text\n",
    "df['time']=times_txt\n",
    "df['name']=names_txt\n",
    "df.rename({0:'msgid',1:'msg'}, axis=1, inplace=True)\n",
    "\n",
    "from datetime import datetime\n",
    "weekdays = {\n",
    "    '일요일': 'Sunday',\n",
    "    '월요일': 'Monday',\n",
    "    '화요일': 'Tuesday',\n",
    "    '수요일': 'Wednesday',\n",
    "    '목요일': 'Thursday',\n",
    "    '금요일': 'Friday',\n",
    "    '토요일': 'Saturday'\n",
    "}\n",
    "\n",
    "df['datetime'] = df.date + \" \" + df.time\n",
    "new_date = []\n",
    "for dt in df['datetime']:\n",
    "    for k in weekdays.keys():\n",
    "        dt = dt.replace(k, weekdays[k])\n",
    "    dt = dt.replace(\"오전\",\"AM\").replace(\"오후\",\"PM\")\n",
    "    new_date.append(dt)\n",
    "df['datetime'] = new_date\n",
    "df['datetime'] = [datetime.strptime(x, f\"%Y년 %m월 %d일 %A %p %I:%M\") for x in df.datetime]\n",
    "\n",
    "# %A: 요일의 전체 이름\n",
    "# %p: AM 또는 PM\n",
    "# %I: 12시간제 시간\n",
    "# %M: 분 (00부터 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text 묶음으로 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain_community.llms import Ollama\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import DataFrameLoader\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 1. 문서 준비하기\n",
    "# texts = df.query('name==\"ㄱ✌️우리 예쁜 민쥬✌️\"')['msg']\n",
    "# texts = texts.reset_index(drop=True)\n",
    "# document = \" \".join(texts)\n",
    "\n",
    "# # # 2. 문서를 작은 조각으로 나누기\n",
    "# # # 이렇게 하면 나중에 필요한 정보를 더 쉽게 찾을 수 있어요\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "#                                                 chunk_overlap=50,\n",
    "#                                                 length_function=len,\n",
    "#                                                 is_seperator_regex=False,) # chunk_size = 문서 조각 1개 내 문자 수\n",
    "# texts = text_splitter.create_documents(document)\n",
    "\n",
    "# # 3. 벡터 데이터베이스 만들기\n",
    "# # 이것은 마치 책의 색인처럼 나중에 정보를 빨리 찾을 수 있게 해줍니다\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "#     model_kwargs={'device':'cuda'},\n",
    "#     encode_kwargs={'normalize_embeddings':True,'batch_size': 32})\n",
    "\n",
    "# with open(file='embeddings_mj.pickle', mode='wb') as f:\n",
    "#     pickle.dump(embeddings, f)\n",
    "\n",
    "# max_batch_size=166\n",
    "# texts = [texts[i:i+max_batch_size] for i in range(0, len(texts), max_batch_size)] # 최대 배치 사이즈 만큼 잘라줌\n",
    "\n",
    "# db = Chroma.from_texts(texts=texts[0],\n",
    "#             embedding=embeddings)\n",
    "\n",
    "# for chunk in tqdm(texts[1:]):\n",
    "#     db.add_texts(texts=chunk, embedding_function=embeddings)\n",
    "\n",
    "# # 4. Llama 모델 준비하기\n",
    "# # 이 모델이 실제로 질문에 답변을 생성할 거예요\n",
    "\n",
    "\n",
    "# # 5. 질문에 답변하는 함수 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df -> documnet로 RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = [str(x) for x in df['datetime']] # documnet 객체에는 str, int, float, bool 만 들어갈 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.memory import ConversationSummaryBufferMemory\n",
    "# from langchain_community.llms import Ollama\n",
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.document_loaders import DataFrameLoader\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # 1. 문서 준비하기\n",
    "tardf = df.copy()\n",
    "\n",
    "# # 2. 문서를 작은 조각으로 나누기\n",
    "# # 이렇게 하면 나중에 필요한 정보를 더 쉽게 찾을 수 있어요\n",
    "loader = DataFrameLoader(tardf, page_content_column='msg')\n",
    "pages = loader.load_and_split()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, # chunk_size: chunk의 최대 크기(length_function로 측정)\n",
    "                                                chunk_overlap=50, # chunk_size = 문서 조각 1개 내 문자 수\n",
    "                                                length_function=len,) \n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# 3. 벡터 데이터베이스 만들기\n",
    "# 이것은 마치 책의 색인처럼 나중에 정보를 빨리 찾을 수 있게 해줍니다\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True,'batch_size': 32})\n",
    "\n",
    "with open(file='embeddings_mj.pickle', mode='wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "max_batch_size=166\n",
    "docs = [docs[i:i+max_batch_size] for i in range(0, len(docs), max_batch_size)] # 최대 배치 사이즈 만큼 잘라줌\n",
    "db = Chroma.from_documents(documents=docs[0],embedding=embeddings)\n",
    "for chunk in tqdm(docs[1:]):\n",
    "    db.add_documents(documents=chunk, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3:8b')\n",
    "def answer_question(question):\n",
    "    # 관련 정보 찾기\n",
    "    docs = db.similarity_search(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # 프롬프트 만들기\n",
    "    # 이것은 Llama 모델에게 어떻게 답변해야 할지 알려주는 지시사항이에요\n",
    "    # prompt = PromptTemplate(\n",
    "    #     input_variables=[\"context\", \"question\"],\n",
    "    #     template=\"\"\"\n",
    "    #     텍스트:{context} \n",
    "    #     질문: {question}\n",
    "    #     텍스트는 남자 '민주'가 여자친구 '은영'에게 보낸 메세지입니다.\n",
    "    #     텍스트를 바탕으로 마치 당신이 '민주' 인 것처럼 질문에 답변해주세요.\n",
    "    #     답변은 한국어로 해주시고, 답변 시 꼭 이 사람의 말투를 똑같이 흉내내주세요.\n",
    "    #     \\n\\n답변:\"\"\"\n",
    "    # )\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "        문서:{context} \n",
    "        질문: {question}\n",
    "        문서를 참고해 답변해주세요. 답변은 반드시 한국어로 해주세요\n",
    "        \\n\\n답변:\"\"\"\n",
    "    )\n",
    "        # 텍스트는 여자 'USER2'가 남자친구 'USER2' 에게 보낸 메세지입니다.\n",
    "        # 텍스트를 바탕으로 마치 당신이 'USER2' 인 것처럼 질문에 답변해주세요.\n",
    "        # 답변은 한국어로 해주시고, USER2의 말투를 똑같이 사용해주세요.\n",
    "    memory = ConversationSummaryBufferMemory(\n",
    "    llm= Ollama(model='llama3:8b',temperature=1), \n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")\n",
    "    def load_memory(input):\n",
    "        print(input)\n",
    "        return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "    # 답변 생성하기\n",
    "    chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm\n",
    "    response = chain.invoke({\"question\":question,\n",
    "                             \"context\":context})\n",
    "\n",
    "    memory.save_context({\"input\": question},\n",
    "                        {\"output\":response})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메타데이터 필터링 구현하기..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"USER2가 먹고싶은 음식이 뭐야. 한글로 대답해줘.\"\n",
    "answer = answer_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 질문하고 답변 받기\n",
    "# question = \"질문: {USER2의 MBTI를 추측해주세요.} 요구사항: {한글로 작성, 추측한 이유 5개. 원문 텍스트를 근거로 상세 설명.} \"\n",
    "answer = answer_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
